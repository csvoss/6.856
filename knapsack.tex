\section{Removable Online Knapsack Problem}
In this section, we present the 2-competitive solution of Han et al. \cite{han} to the removable online knapsack problem as well as use Yao's Principle show $1+1/e$ is a lower bound on the competitiveness of any algorithm against an oblivious adversary.

The knapsack problem asks: Given a set of items $e_i$ with weights $w_i$ and values $v_i$, select a subset with the maximum sum of values such that the sum of the weights is at most the capacity of the knapsack. We call this the value of the knapsack. The offline version is a classic NP-hard optimization problem, though it has a simple dynamic programming pseudo-polynomial solution. Here, we consider the \emph{online} version, meaning each item is given to the algorithm after the algorithm makes a decision on the previous item. \emph{Removable} means the knapsack is a working set, from which we can remove items and add new items, but not add previously removed items. At all times, the knapsack is limited by its capacity. The objective is to maximize the value of the final knapsack. We say an algorithm is $\alpha$-competitive if the expected value of the final knapsack is at least $\frac{1}{\alpha}$ times the expected value of the optimal knapsack.

We assume the knapsack has capacity 1. This is valid because only weights relative to the knapsack's capacity matters for the capacity constraint, so we can always scale down to a unit capacity.

\subsection{Upper Bound} % Probably will shorten this subsection
To prove an upper bound on the competitiveness of algorithms which yield a solution to the problem, we present a 2-competitive randomized algorithm.

Since the online nature of the problem is what makes it impossible to always achieve the optimal solution, it is insightful to first consider algorithms which behave similarly in both online and offline situations. We look at $MAX$ and $GREEDY$.

$MAX$ simply selects the item with the largest value whose weight does not exceed 1. In the offline case, we can process each item in an arbitrary order and keep track of the best item so far. In the online case, we can do exactly the same with the order given to us, storing the best item so far in our knapsack.

$GREEDY$ selects the $k$ items with the highest ratios of value to weight $\frac{v_i}{w_i}$, maximizing $k$ while keeping the sum of the weights of the selected items at most 1. In the offline case, this can be done trivially by sorting the items by ratio and then selecting the highest $k$ items. An online algorithm can perform at least as well by keeping a working set of the items with the highest ratio while processing each input.
%We demonstrate that a set at least as good is found in the online case by the following algorithm.

\begin{algorithm}
	\caption{online $GREEDY$}
	$S \leftarrow \emptyset$\;
	\ForEach{item $e_i$ with $w_i\le1$, in order of arrival}{
		$S \leftarrow S \cup \{e_i\}$\;
		\While{$\sum_{e_j \in S} w_j > 1$}{
			Remove the item from $S$ with the smallest ratio\;
		}
	}
\end{algorithm}

%We claim every item selected in the offline $GREEDY$ algorithm is selected in the online $GREEDY$ algorithm. Consider the $k$ items the offline algorithm selected. Each
Since when the sum of the weights in $S$ is greater than 1, an item with ratio less than that of the $k$ items with the largest ratios must be in $S$, the $k$ items will never be removed. Thus $S$ is a superset of the items the offline $GREEDY$ algorithm selects.

Neither of these algorithms alone are competitive. Consider an input sequence $\{(w_i,v_i)\}$ of $\{(\epsilon,2\epsilon),\ (\epsilon,\epsilon),\ (\epsilon,\epsilon),\ \ldots\}$ on the $MAX$ algorithm and $\{(\epsilon, 2\epsilon),\ (1,1)\}$ on the $GREEDY$ algorithm. In fact, no deterministic online algorithm is competitive with a constant factor \cite{iwama}. However, using both with randomness can yield a competitive algorithm.

\begin{theorem}
	\emph{\cite{han}}
	Running $MAX$ and $GREEDY$ uniformly at random is a 2-competitive algorithm.
\end{theorem}
\begin{proof}
	For a given input sequence $T$, let $GREEDY(T)$, $MAX(T)$, and $OPT(T)$ be the values of the sets returned by $GREEDY$, $MAX$, and an optimal solution, respectively. Consider the fractional knapsack solution to the offline version, that is, when we are allowed to take fractional parts of items. Then a greedy solution is optimal: Sort by ratio and take the highest ratio items until the knapsack is full, potentially taking only part of the last item in the knapsack. The fractional knapsack problem is a relaxation of the integral knapsack problem, so the fractional solution is not worse than the integral solution.
	
	Now the offline $GREEDY$ algorithm by definition selects all the items in the fractional solution except the final fractional item (if one exists). Thus the online $GREEDY$ algorithm also selects all except the fractional item. Finally the $MAX$ algorithm selects an item with value at least that of the fractional item, so $GREEDY(T)+MAX(T) \ge OPT(T)$. Thus the competitive ratio is $\frac{2\cdot OPT(T)}{GREEDY(T)+MAX(T)} \le 2$.
\end{proof}

\subsection{Lower Bound}
We use Yao's Principle to prove a lower bound of $1+1/e$ for the competitive ratio of any removable online knapsack algorithm.

To do so, we must find a distribution of inputs for which no deterministic algorithm performs ``well'', which means the expected value of the output from any deterministic algorithm is at most $\frac{1}{1+1/e}$ times the expected optimum. As the power of an offline algorithm is in the additional information it has over an online algorithm, it is reasonable that in constructing our ``bad'' input, we would want to limit the information the algorithm has about future items. Thus it is not surprising that we will construct a family of inputs where each input is a prefix of the same base sequence.

Let the probability of $k+1$ items be $p_k$. For simplicity in analysis, we will let the first item $e_0=(w_0,v_0)=(1,1)$ with future items having much smaller weights and values but a higher ratio so that the problem is essentially reduced to determining when to remove the first item from the knapsack.

This is reminiscent of the \emph{ski rental problem}, one of the most well-understood online problems \cite{karlin}, where a client must choose whether and when to buy skis for a fixed cost versus paying a smaller cost each day. Our knapsack problem is the reverse of this: instead of deciding when to switch from renting skis to buying skis to minimize total cost, we are deciding when to switch from having a single large item in our knapsack to getting a stream of smaller items with a better ratio to maximize total value.

Finally, we want the expected value of removing $e_0$ and selecting all following items to be close to 1 at any given point. A simple way to achieve this is to let the remaining items have a constant value and have the $p_k$ form an exponential distribution.

This leads us towards the input sequence given by Han et al. \cite{han}:
\begin{equation}
	\label{eq:knapsack_dist}
	(1,1),\ \underbrace{(1/n^2,1/n),\ (1/n^2,1/n),\ldots\ (1/n^2,1/n)}_{k \text{ terms}}
\end{equation}
for a given value of $n$ and where there are $k+1$ items with probability $p_k = \frac{1-e^{-1/n}}{1-e^{-n}} \cdot e^{-(k-1)/n}$ for $k=1,2,\ldots,n^2$.

\begin{theorem}
	\emph{\cite{han}}
	No online algorithm has competitive ratio less than $1+1/e$ for the removable online knapsack problem.
\end{theorem}
\begin{proof}
	We apply Yao's Principle on the input distribution \eqref{eq:knapsack_dist}. The optimal strategy with full information is to keep $e_0$ for $k\le n$ and remove $e_0$ immediately otherwise. This leads us to the expected optimal value
	\[ \sum_{i=1}^n 1\cdot p_i + \sum_{i=n+1}^{n^2} \frac{i}{n}\cdot p_i = \frac{1-e^{-1/n}}{1-e^{-n}} \left( \sum_{i=1}^n e^{-(i-1)/n} + \frac{1}{n} \sum_{i=n+1}^{n^2} i\cdot e^{-(i-1)/n} \right) \]
	which has value $1+1/e$ as $n \rightarrow \infty$.

	Since on this set of inputs, the initial item has weight 1 and the sum of the rest of the items is at most 1, any optimal algorithm having removed the initial item can perform a greedy algorithm and put all future items in the knapsack. Thus an optimal online deterministic algorithm can only decide how many items it must see before it removes $e_0$. For inputs $k<l$, the algorithm keeps $e_0$ and removes all incoming items, so it obtains value 1. For $k\ge l$, the algorithm removes $e_0$ when the $l$\textsuperscript{th} item appears, and accumulates items $e_l$ through $e_k$. Then the expected value of this algorithm is
	\[ \sum_{i=1}^{l-1} 1\cdot p_i + \sum_{i=l}^{n^2} \frac{i-l}{n}\cdot p_i = \frac{1-e^{-1/n}}{1-e^{-n}} \left( \sum_{i=1}^{l-1} e^{-(i-1)/n} + \frac{1}{n} \sum_{i=l}^{n^2} (i-1)\cdot e^{-(i-1)/n} \right) \]
which has value 1 as $n \rightarrow \infty$.

By Yao's Principle, since every deterministic algorithm has finds $(1+1/e)^{-1}$ of the optimum set on this distribution of inputs, any online algorithm performs at least as poorly. Thus any online algorithm to this problem has competitive ratio at least $1+1/e$.
\end{proof}

%\subsection{Remarks}
%Yao's Principle is used here to give a lower bound on the competitiveness of algorithms for the removable online knapsack problem, which is an upper bound on their effectiveness. It can be used to show the tightness of a particular solution.

%Also, as we saw here, there is a dual between using an algorithm to demonstrate a lower bound on the solvability of a problem (alternatively, an upper bound on the hardness of a problem), and using a series of inputs to demonstrate an upper bound on the solvability of a problem (alternatively, an lower bound on the hardness). As more work is done on a particular problem, the gap between the two shrink until a provably optimal algorithm is found.
