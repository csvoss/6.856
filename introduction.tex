\newcommand{\Expected}{\displaystyle\mathop{\mathds{E}}}
\newcommand{\argmin}{\displaystyle\mathop{\text{arg\;min}}}

\section{Introduction}

Yao's minimax principle, proposed by Yao in 1977, was first used to study lower bounds on the optimal running times of randomized algorithms for a particular problem. In this paper, we survey a series of games for which Yao's principle can be used to bound the effectiveness of all randomized algorithms against an adversary.
In this section, we present Yao's Principle and give a proof of it.

But first, we clarify the definition of a randomized algorithm.

\begin{description}
	\item[Randomized Algorithm] An algorithm which has the capabilities of a deterministic algorithm but with additional access to a stream of uniformly random bits.
\end{description}

Thus we can model a randomized algorithm as a deterministic algorithm with an additional secondary input of a stream of random bits. Conditioned on this stream, this is an entirely deterministic algorithm. Thus a randomized algorithm is modeled by a set of deterministic algorithms from which the randomized algorithm chooses at runtime with some probability distribution, before seeing the primary input.

%\subsection{Yao's Principle}

\begin{description}
	\item[Yao's Minimax Principle] \emph{The best deterministic algorithm for any particular distribution of inputs runs at least as well as any randomized algorithm when run on its own worst-case input}.
\end{description}
More formally,

$$\min_{r \in \mathcal{R}} \Expected_{x \in X} \texttt{cost}(r, x) \leq \max_{x \in \mathcal{X}} \Expected_{r \in R} \texttt{cost}(r, x)$$

where \begin{align*}
\mathcal{X} =&\; \text{the set of all possible inputs to the algorithm.}
\\
\mathcal{R} =&\; \text{the set of all possible strings defining deterministic algorithms.}
\\
X =&\; \text{any probability distribution over inputs to the algorithm.}
\\
R =&\; \text{\parbox[t]{5.5in}{any randomized algorithm, ie., any probability distribution over strings defining deterministic algorithms.}}
\\
\texttt{cost}(r, x) =&\; \text{\parbox[t]{5.5in}{the cost (often running time but in this paper will be an optimization quantity) of the algorithm $r$ running on input $x$.}}
\end{align*}

In other words, if you can figure out how well the best deterministic algorithm runs against any distribution $X$ of random inputs, you know that any randomized algorithm you formulate can do no better against its worst-case input -- giving you a lower bound on the randomized algorithm's running time.
Although $X$ need not be the worst possible distribution over random inputs for this principle to be useful, note that the worse $X$ is as an input distribution, the tighter the resulting lower bound will be. 

To further develop the intuition behind Yao's minimax principle, we will briefly cover its proof.

\subsection{Direct Proof of Yao's Principle}

Consider an arbitrary input distribution $X$. Let $d_X$ be the best deterministic algorithm for input distribution $X$, that is, the deterministic algorithm which minimizes expected cost $\Expected_{x\in X}\texttt{cost}(d_X,x)$.

Now since any randomized algorithm $R$ can be modeled as a distribution of deterministic algorithms, the expected cost of $R$ on $X$ is $\Expected_{x\in X}\Expected_{r\in R}\texttt{cost}(r,x)$, or $\Expected_{r\in R}\Expected_{x\in X}\texttt{cost}(r,x)$ by linearity of expectation. This is the weighted average of the expected costs of the deterministic algorithms in the distribution of $R$. Since the cost of each of these is at least $\Expected_{x\in X}\texttt{cost}(d_X,x)$, we have
\[\Expected_{r\in R}\Expected_{x\in X}\texttt{cost}(r,x) \ge \Expected_{x\in X}\texttt{cost}(d_X,x)\]

The cost of $R$ on its worst-case input cannot be greater than the cost of $R$ on this input distribution. Then
\[\max_{x\in\mathcal{X}}\Expected_{r\in R}\texttt{cost}(r,x)
\ge \Expected_{x\in X}\Expected_{r\in R}\texttt{cost}(r,x)
= \Expected_{r\in R}\Expected_{x\in X}\texttt{cost}(r,x)
\ge \Expected_{x\in X}\texttt{cost}(d_X,x)\]

Now $d_X$ is simply $\argmin_{r\in\mathcal{R}}\Expected_{x\in X}\texttt{cost}(r,x)$ by definition. Thus we have
\[\max_{x\in\mathcal{X}}\Expected_{r\in R}\texttt{cost}(r,x) \ge \min_{r\in\mathcal{R}}\Expected_{x\in X}\texttt{cost}(r,x)\]

\subsection{Proof of Yao's Principle}

Consider a two-player game, as defined in game theory. Each player $i$ has a set of moves $M_i$ that they may choose from. The game is defined by a \emph{payoff matrix} of outcomes for each player, a function mapping $M_1 \times M_2 \rightarrow \mathds{R} \times \mathds{R}$.

In a \emph{pure strategy}, player $1$ chooses exactly one move $x$ from $M_1$ to play. In a \emph{mixed strategy}, player $1$ instead chooses a probability distribution $\pi_1(x)$ of moves from $M_1$.

Let $\texttt{payoff}_1(x, y)$ denote the payoff to player 1 when the players choose moves $x$ and $y$, respectively.

%% \textbf{Lemma:} If you ``announce'' your mixed strategy -- thus committing to not change it -- then there exists a pure strategy for me that does at least as good as any mixed strategy for me. That is:

%% $$\forall \pi_1, \pi_2, \exists x_{opt}, \Expected_{y \in \pi_2} \texttt{payoff}_1(x_{opt}, y) \geq \Expected_{y \in \pi_2, x \in \pi_1} \texttt{payoff}_1(x, y)$$

%% \begin{proof}

%% Each component move $x_i$ of $\pi_1$ contributes a component $\pi_1(x_i) \texttt{payoff}_1(x_i, y)$ to the sum $\Expected_{x \in \pi_1} \texttt{payoff}_1(x, y)$. There exists some move $x_{opt}$ for which $\texttt{payoff}_1(x_i, y)$ is maximized. If we choose that $x_{opt}$ as a pure strategy instead, then $\Expected_{y \in \pi_2} \texttt{payoff}_1(x_{opt}, y) \geq \Expected_{y \in \pi_2, x \in \pi_1} \texttt{payoff}_1(x, y)$ as desired.

%% \end{proof}

This leads to von Neumann's minimax theorem,

$$\max_{A \in \mathcal{A}} \min_{B \in \mathcal{B}} \Expected_{a \in A, b \in B} \texttt{payoff}_1(a, b) = \min_{B \in \mathcal{B}} \max_{A \in \mathcal{A}} \Expected_{a \in A, b \in B} \texttt{payoff}_1(a, b)$$


where \begin{align*}
\mathcal{A} =&\; \text{set of moves available to player 1.}
\\
\mathcal{B} =&\; \text{set of moves available to player 2.}
\\
A =&\; \text{any probability distribution over moves to player 1}
    \\&\text{-- that is, any mixed strategy for player 1.}
\\
B =&\; \text{any probability distribution over moves to player 2}
    \\&\text{ -- that is, any mixed strategy for player 2.}
\\
\texttt{payoff}_1(a, b) =&\; \text{the payoff to player 1}
\end{align*}

This in turn can be rewritten to yield the following:

$$\max_{a \in \mathcal{A}} \Expected_{b \in B} \texttt{payoff}_1(a, b) \geq \max_{b \in \mathcal{B}} \Expected_{a \in A} \texttt{payoff}_1(a, b)$$

for any mixed strategies $A$ and $B$.

Finally, given this inequality, we need only observe the following correspondence between algorithms and strategies and between inputs and strategies in order to yield Yao's principle, as follows:

$$\min_{r \in \mathcal{R}} \Expected_{x \in X} \texttt{cost}(r, x) \leq \max_{x \in \mathcal{X}} \Expected_{r \in R} \texttt{cost}(r, x)$$

for any distribution $X$ of inputs and any distribution $R$ of randomized algorithms.

\begin{itemize}

\item{The set of moves for player 1 ($\mathcal{A}$) is the same as the set of deterministic algorithms ($\mathcal{R}$).}
\item{The set of moves for player 2 ($\mathcal{B}$) is the same as the set of possible inputs ($\mathcal{X}$).}
\item{A mixed strategy for player 1 ($A$) is the same as a randomized algorithm -- that is, a probability distribution over deterministic algorithms ($R$).}
\item{A mixed strategy for player 2 ($B$) is the same as a probability distribution over inputs to the algorithm ($X$).}
\item{The payoff to player 1 ($\texttt{payoff}$) is the negated cost to player 1 ($-\texttt{cost}$).}

\end{itemize}


%% The following table summarizes each of the parts of Yao's principle, and their roles. As we demonstrate examples which utilize Yao's principle, this table will be used in order to clarify how the principle applies to each new situation.

%% \begin{tabular}{r l} %% We can definitely change this table. This is just a first draft.
%% Randomized algorithm chooses: & some distribution $R$ over strings $r$ as its source of randomness
%% \\
%% Deterministic algorithm chooses: & some fixed string $r$
%% \\
%% Mixed-strategy adversary chooses: & some distribution $X$ over inputs $x$ for the algorithm
%% \\
%% Pure-strategy adversary chooses: & some fixed input $x$
%% \\
%% Adversary wants to maximize: & the running time of the algorithm, $\texttt{cost}(r,x)$
%% \end{tabular}
